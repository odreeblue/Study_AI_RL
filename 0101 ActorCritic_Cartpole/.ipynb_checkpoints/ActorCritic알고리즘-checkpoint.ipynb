{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242ce636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Actor(Model):\n",
    "    def __init__(self, state_size: int, action_size: int, \n",
    "    ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # set the hidden layers\n",
    "        self.layer1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.layer2 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.policy = tf.keras.layers.Dense(self.action_size,activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        layer1 = self.layer1(state)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        policy = self.policy(layer2)\n",
    "        return policy\n",
    "    \n",
    "class CriticV(Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_size: int, \n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(CriticV, self).__init__()\n",
    "        self.layer1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.layer2 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.value = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "    def call(self, state):\n",
    "        layer1 = self.layer1(state)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        value = self.value(layer2)\n",
    "        return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ae92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"A2CAgent interacting with environment.\n",
    "        \n",
    "    Attributes:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        gamma (float): discount factor\n",
    "        entropy_weight (float): rate of weighting entropy into the loss function\n",
    "        actor (tf.keras.Model): target actor model to select actions\n",
    "        critic (tf.keras.Model): critic model to predict state values\n",
    "        actor_optimizer (optim.Optimizer) : optimizer of actor\n",
    "        critic_optimizer (optim.Optimizer) : optimizer of critic\n",
    "        transition (list): temporory storage for the recent transition\n",
    "        is_test (bool): flag to show the current mode (train / test)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # CREATING THE Q-Network\n",
    "        self.env = env\n",
    "        \n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        print(\"state_size: \",self.state_size)\n",
    "        print(\"action_size: \",self.action_size)\n",
    "        \n",
    "        self.actor_lr = 7e-3\n",
    "        self.critic_lr = 7e-3\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        self.actor = Actor(self.state_size, self.action_size\n",
    "                          )\n",
    "        self.critic = CriticV(self.state_size\n",
    "                          )\n",
    "        # self.a_opt = tf.keras.optimizers.RMSprop(learning_rate=1e-5)\n",
    "        # self.c_opt = tf.keras.optimizers.RMSprop(learning_rate=1e-5)\n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=self.actor_lr)\n",
    "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=self.critic_lr)\n",
    "        self.log_prob = None\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        prob = self.actor(np.array([state]))\n",
    "        prob = prob.numpy()\n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0])\n",
    "    \n",
    "    def actor_loss(self, prob, action, TD):\n",
    "        \n",
    "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob*TD\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = np.array([state])\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            \n",
    "            curr_P = self.actor(state, training=True)\n",
    "            curr_Q = self.critic(state,training=True)\n",
    "            next_Q = self.critic(next_state, training=True)\n",
    "            \n",
    "            expected_Q = reward + self.gamma*next_Q*(1-int(done))\n",
    "            TD = expected_Q - curr_Q\n",
    "            \n",
    "            # critic loss\n",
    "            critic_loss = tf.keras.losses.MSE(expected_Q, curr_Q)\n",
    "            \n",
    "            actor_loss = self.actor_loss(curr_P, action, TD)\n",
    "            \n",
    "        actorGrads = tape1.gradient(actor_loss,  self.actor.trainable_variables)\n",
    "        criticGrads = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(actorGrads, self.actor.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(criticGrads, self.critic.trainable_variables))\n",
    "        \n",
    "        return actor_loss, critic_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0e2055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 9.0\n",
      "Episode 2: 9.0\n",
      "Episode 3: 9.0\n",
      "Episode 4: 9.0\n",
      "Episode 5: 10.0\n",
      "Episode 6: 10.0\n",
      "Episode 7: 8.0\n",
      "Episode 8: 9.0\n",
      "Episode 9: 9.0\n",
      "Episode 10: 10.0\n",
      "Episode 11: 9.0\n",
      "Episode 12: 9.0\n",
      "Episode 13: 9.0\n",
      "Episode 14: 10.0\n",
      "Episode 15: 8.0\n",
      "Episode 16: 9.0\n",
      "Episode 17: 8.0\n",
      "Episode 18: 10.0\n",
      "Episode 19: 9.0\n",
      "Episode 20: 10.0\n",
      "Episode 21: 9.0\n",
      "Episode 22: 9.0\n",
      "Episode 23: 9.0\n",
      "Episode 24: 10.0\n",
      "Episode 25: 8.0\n",
      "Episode 26: 9.0\n",
      "Episode 27: 10.0\n",
      "Episode 28: 8.0\n",
      "Episode 29: 9.0\n",
      "Episode 30: 10.0\n",
      "Episode 31: 9.0\n",
      "Episode 32: 9.0\n",
      "Episode 33: 8.0\n",
      "Episode 34: 10.0\n",
      "Episode 35: 9.0\n",
      "Episode 36: 9.0\n",
      "Episode 37: 8.0\n",
      "Episode 38: 10.0\n",
      "Episode 39: 11.0\n",
      "Episode 40: 9.0\n",
      "Episode 41: 10.0\n",
      "Episode 42: 9.0\n",
      "Episode 43: 9.0\n",
      "Episode 44: 10.0\n",
      "Episode 45: 9.0\n",
      "Episode 46: 9.0\n",
      "Episode 47: 10.0\n",
      "Episode 48: 10.0\n",
      "Episode 49: 10.0\n",
      "Episode 50: 9.0\n",
      "Episode 51: 9.0\n",
      "Episode 52: 10.0\n",
      "Episode 53: 9.0\n",
      "Episode 54: 8.0\n",
      "Episode 55: 10.0\n",
      "Episode 56: 10.0\n",
      "Episode 57: 10.0\n",
      "Episode 58: 9.0\n",
      "Episode 59: 10.0\n",
      "Episode 60: 10.0\n",
      "Episode 61: 8.0\n",
      "Episode 62: 8.0\n",
      "Episode 63: 11.0\n",
      "Episode 64: 10.0\n",
      "Episode 65: 10.0\n",
      "Episode 66: 10.0\n",
      "Episode 67: 8.0\n",
      "Episode 68: 10.0\n",
      "Episode 69: 10.0\n",
      "Episode 70: 10.0\n",
      "Episode 71: 11.0\n",
      "Episode 72: 9.0\n",
      "Episode 73: 9.0\n",
      "Episode 74: 9.0\n",
      "Episode 75: 9.0\n",
      "Episode 76: 9.0\n",
      "Episode 77: 10.0\n",
      "Episode 78: 8.0\n",
      "Episode 79: 8.0\n",
      "Episode 80: 9.0\n",
      "Episode 81: 9.0\n",
      "Episode 82: 9.0\n",
      "Episode 83: 9.0\n",
      "Episode 84: 10.0\n",
      "Episode 85: 10.0\n",
      "Episode 86: 9.0\n",
      "Episode 87: 10.0\n",
      "Episode 88: 9.0\n",
      "Episode 89: 10.0\n",
      "Episode 90: 9.0\n",
      "Episode 91: 10.0\n",
      "Episode 92: 10.0\n",
      "Episode 93: 9.0\n",
      "Episode 94: 8.0\n",
      "Episode 95: 9.0\n",
      "Episode 96: 9.0\n",
      "Episode 97: 10.0\n",
      "Episode 98: 9.0\n",
      "Episode 99: 8.0\n",
      "Episode 100: 9.0\n",
      "Episode 101: 8.0\n",
      "Episode 102: 10.0\n",
      "Episode 103: 8.0\n",
      "Episode 104: 10.0\n",
      "Episode 105: 9.0\n",
      "Episode 106: 10.0\n",
      "Episode 107: 8.0\n",
      "Episode 108: 9.0\n",
      "Episode 109: 9.0\n",
      "Episode 110: 9.0\n",
      "Episode 111: 9.0\n",
      "Episode 112: 9.0\n",
      "Episode 113: 9.0\n",
      "Episode 114: 9.0\n",
      "Episode 115: 10.0\n",
      "Episode 116: 9.0\n",
      "Episode 117: 10.0\n",
      "Episode 118: 9.0\n",
      "Episode 119: 11.0\n",
      "Episode 120: 9.0\n",
      "Episode 121: 9.0\n",
      "Episode 122: 10.0\n",
      "Episode 123: 10.0\n",
      "Episode 124: 10.0\n",
      "Episode 125: 9.0\n",
      "Episode 126: 11.0\n",
      "Episode 127: 8.0\n",
      "Episode 128: 8.0\n",
      "Episode 129: 8.0\n",
      "Episode 130: 9.0\n",
      "Episode 131: 9.0\n",
      "Episode 132: 11.0\n",
      "Episode 133: 8.0\n",
      "Episode 134: 9.0\n",
      "Episode 135: 8.0\n",
      "Episode 136: 8.0\n",
      "Episode 137: 10.0\n",
      "Episode 138: 9.0\n",
      "Episode 139: 8.0\n",
      "Episode 140: 9.0\n",
      "Episode 141: 10.0\n",
      "Episode 142: 10.0\n",
      "Episode 143: 9.0\n",
      "Episode 144: 9.0\n",
      "Episode 145: 10.0\n",
      "Episode 146: 10.0\n",
      "Episode 147: 10.0\n",
      "Episode 148: 9.0\n",
      "Episode 149: 10.0\n",
      "Episode 150: 10.0\n",
      "Episode 151: 9.0\n",
      "Episode 152: 10.0\n",
      "Episode 153: 10.0\n",
      "Episode 154: 9.0\n",
      "Episode 155: 10.0\n",
      "Episode 156: 10.0\n",
      "Episode 157: 8.0\n",
      "Episode 158: 9.0\n",
      "Episode 159: 10.0\n",
      "Episode 160: 8.0\n",
      "Episode 161: 10.0\n",
      "Episode 162: 10.0\n",
      "Episode 163: 9.0\n",
      "Episode 164: 9.0\n",
      "Episode 165: 9.0\n",
      "Episode 166: 9.0\n",
      "Episode 167: 10.0\n",
      "Episode 168: 8.0\n",
      "Episode 169: 11.0\n",
      "Episode 170: 10.0\n",
      "Episode 171: 10.0\n",
      "Episode 172: 8.0\n",
      "Episode 173: 10.0\n",
      "Episode 174: 9.0\n",
      "Episode 175: 10.0\n",
      "Episode 176: 8.0\n",
      "Episode 177: 9.0\n",
      "Episode 178: 10.0\n",
      "Episode 179: 10.0\n",
      "Episode 180: 8.0\n",
      "Episode 181: 9.0\n",
      "Episode 182: 10.0\n",
      "Episode 183: 10.0\n",
      "Episode 184: 10.0\n",
      "Episode 185: 9.0\n",
      "Episode 186: 9.0\n",
      "Episode 187: 10.0\n",
      "Episode 188: 10.0\n",
      "Episode 189: 10.0\n",
      "Episode 190: 10.0\n",
      "Episode 191: 9.0\n",
      "Episode 192: 9.0\n",
      "Episode 193: 10.0\n",
      "Episode 194: 9.0\n",
      "Episode 195: 8.0\n",
      "Episode 196: 10.0\n",
      "Episode 197: 10.0\n",
      "Episode 198: 10.0\n",
      "Episode 199: 10.0\n",
      "Episode 200: 8.0\n",
      "Episode 201: 10.0\n",
      "Episode 202: 11.0\n",
      "Episode 203: 10.0\n",
      "Episode 204: 9.0\n",
      "Episode 205: 10.0\n",
      "Episode 206: 10.0\n",
      "Episode 207: 10.0\n",
      "Episode 208: 8.0\n",
      "Episode 209: 10.0\n",
      "Episode 210: 8.0\n",
      "Episode 211: 9.0\n",
      "Episode 212: 9.0\n",
      "Episode 213: 9.0\n",
      "Episode 214: 10.0\n",
      "Episode 215: 9.0\n",
      "Episode 216: 10.0\n",
      "Episode 217: 11.0\n",
      "Episode 218: 10.0\n",
      "Episode 219: 8.0\n",
      "Episode 220: 9.0\n",
      "Episode 221: 10.0\n",
      "Episode 222: 8.0\n",
      "Episode 223: 10.0\n",
      "Episode 224: 11.0\n",
      "Episode 225: 10.0\n",
      "Episode 226: 8.0\n",
      "Episode 227: 9.0\n",
      "Episode 228: 10.0\n",
      "Episode 229: 9.0\n",
      "Episode 230: 9.0\n",
      "Episode 231: 10.0\n",
      "Episode 232: 9.0\n",
      "Episode 233: 11.0\n",
      "Episode 234: 8.0\n",
      "Episode 235: 9.0\n",
      "Episode 236: 10.0\n",
      "Episode 237: 11.0\n",
      "Episode 238: 10.0\n",
      "Episode 239: 22.0\n",
      "Episode 240: 18.0\n",
      "Episode 241: 40.0\n",
      "Episode 242: 38.0\n",
      "Episode 243: 24.0\n",
      "Episode 244: 14.0\n",
      "Episode 245: 27.0\n",
      "Episode 246: 36.0\n",
      "Episode 247: 46.0\n",
      "Episode 248: 35.0\n",
      "Episode 249: 29.0\n",
      "Episode 250: 22.0\n",
      "Episode 251: 21.0\n",
      "Episode 252: 23.0\n",
      "Episode 253: 17.0\n",
      "Episode 254: 35.0\n",
      "Episode 255: 20.0\n",
      "Episode 256: 16.0\n",
      "Episode 257: 14.0\n",
      "Episode 258: 16.0\n",
      "Episode 259: 13.0\n",
      "Episode 260: 15.0\n",
      "Episode 261: 13.0\n",
      "Episode 262: 12.0\n",
      "Episode 263: 17.0\n",
      "Episode 264: 16.0\n",
      "Episode 265: 21.0\n",
      "Episode 266: 57.0\n",
      "Episode 267: 33.0\n",
      "Episode 268: 21.0\n",
      "Episode 269: 28.0\n",
      "Episode 270: 25.0\n",
      "Episode 271: 28.0\n",
      "Episode 272: 31.0\n",
      "Episode 273: 40.0\n",
      "Episode 274: 35.0\n",
      "Episode 275: 17.0\n",
      "Episode 276: 24.0\n",
      "Episode 277: 50.0\n",
      "Episode 278: 46.0\n",
      "Episode 279: 47.0\n",
      "Episode 280: 26.0\n",
      "Episode 281: 68.0\n",
      "Episode 282: 54.0\n",
      "Episode 283: 111.0\n",
      "Episode 284: 49.0\n",
      "Episode 285: 31.0\n",
      "Episode 286: 32.0\n",
      "Episode 287: 30.0\n",
      "Episode 288: 47.0\n",
      "Episode 289: 33.0\n",
      "Episode 290: 51.0\n",
      "Episode 291: 36.0\n",
      "Episode 292: 49.0\n",
      "Episode 293: 24.0\n",
      "Episode 294: 42.0\n",
      "Episode 295: 47.0\n",
      "Episode 296: 23.0\n",
      "Episode 297: 39.0\n",
      "Episode 298: 60.0\n",
      "Episode 299: 33.0\n",
      "Episode 300: 41.0\n",
      "Episode 301: 35.0\n",
      "Episode 302: 36.0\n",
      "Episode 303: 54.0\n",
      "Episode 304: 28.0\n",
      "Episode 305: 26.0\n",
      "Episode 306: 21.0\n",
      "Episode 307: 24.0\n",
      "Episode 308: 20.0\n",
      "Episode 309: 17.0\n",
      "Episode 310: 15.0\n",
      "Episode 311: 15.0\n",
      "Episode 312: 23.0\n",
      "Episode 313: 20.0\n",
      "Episode 314: 17.0\n",
      "Episode 315: 24.0\n",
      "Episode 316: 21.0\n",
      "Episode 317: 26.0\n",
      "Episode 318: 20.0\n",
      "Episode 319: 24.0\n",
      "Episode 320: 23.0\n",
      "Episode 321: 20.0\n",
      "Episode 322: 19.0\n",
      "Episode 323: 19.0\n",
      "Episode 324: 24.0\n",
      "Episode 325: 26.0\n",
      "Episode 326: 24.0\n",
      "Episode 327: 18.0\n",
      "Episode 328: 18.0\n",
      "Episode 329: 18.0\n",
      "Episode 330: 21.0\n",
      "Episode 331: 19.0\n",
      "Episode 332: 31.0\n",
      "Episode 333: 20.0\n",
      "Episode 334: 24.0\n",
      "Episode 335: 27.0\n",
      "Episode 336: 19.0\n",
      "Episode 337: 29.0\n",
      "Episode 338: 21.0\n",
      "Episode 339: 19.0\n",
      "Episode 340: 23.0\n",
      "Episode 341: 17.0\n",
      "Episode 342: 17.0\n",
      "Episode 343: 15.0\n",
      "Episode 344: 17.0\n",
      "Episode 345: 15.0\n",
      "Episode 346: 15.0\n",
      "Episode 347: 23.0\n",
      "Episode 348: 22.0\n",
      "Episode 349: 21.0\n",
      "Episode 350: 15.0\n",
      "Episode 351: 17.0\n",
      "Episode 352: 21.0\n",
      "Episode 353: 23.0\n",
      "Episode 354: 14.0\n",
      "Episode 355: 26.0\n",
      "Episode 356: 19.0\n",
      "Episode 357: 20.0\n",
      "Episode 358: 29.0\n",
      "Episode 359: 25.0\n",
      "Episode 360: 32.0\n",
      "Episode 361: 57.0\n",
      "Episode 362: 35.0\n",
      "Episode 363: 37.0\n",
      "Episode 364: 29.0\n",
      "Episode 365: 30.0\n",
      "Episode 366: 40.0\n",
      "Episode 367: 34.0\n",
      "Episode 368: 27.0\n",
      "Episode 369: 66.0\n",
      "Episode 370: 41.0\n",
      "Episode 371: 24.0\n",
      "Episode 372: 27.0\n",
      "Episode 373: 38.0\n",
      "Episode 374: 39.0\n",
      "Episode 375: 27.0\n",
      "Episode 376: 52.0\n",
      "Episode 377: 59.0\n",
      "Episode 378: 68.0\n",
      "Episode 379: 46.0\n",
      "Episode 380: 34.0\n",
      "Episode 381: 39.0\n",
      "Episode 382: 76.0\n",
      "Episode 383: 68.0\n",
      "Episode 384: 47.0\n",
      "Episode 385: 54.0\n",
      "Episode 386: 33.0\n",
      "Episode 387: 76.0\n",
      "Episode 388: 32.0\n",
      "Episode 389: 35.0\n",
      "Episode 390: 96.0\n",
      "Episode 391: 44.0\n",
      "Episode 392: 38.0\n",
      "Episode 393: 56.0\n",
      "Episode 394: 42.0\n",
      "Episode 395: 61.0\n",
      "Episode 396: 43.0\n",
      "Episode 397: 58.0\n",
      "Episode 398: 77.0\n",
      "Episode 399: 64.0\n",
      "Episode 400: 57.0\n",
      "Episode 401: 75.0\n",
      "Episode 402: 75.0\n",
      "Episode 403: 49.0\n",
      "Episode 404: 73.0\n",
      "Episode 405: 100.0\n",
      "Episode 406: 53.0\n",
      "Episode 407: 28.0\n",
      "Episode 408: 32.0\n",
      "Episode 409: 38.0\n",
      "Episode 410: 28.0\n",
      "Episode 411: 35.0\n",
      "Episode 412: 45.0\n",
      "Episode 413: 29.0\n",
      "Episode 414: 38.0\n",
      "Episode 415: 61.0\n",
      "Episode 416: 91.0\n",
      "Episode 417: 40.0\n",
      "Episode 418: 49.0\n",
      "Episode 419: 52.0\n",
      "Episode 420: 41.0\n",
      "Episode 421: 44.0\n",
      "Episode 422: 42.0\n",
      "Episode 423: 147.0\n",
      "Episode 424: 86.0\n",
      "Episode 425: 95.0\n",
      "Episode 426: 200.0\n",
      "Episode 427: 200.0\n",
      "Episode 428: 198.0\n",
      "Episode 429: 124.0\n",
      "Episode 430: 157.0\n",
      "Episode 431: 69.0\n",
      "Episode 432: 93.0\n",
      "Episode 433: 96.0\n",
      "Episode 434: 129.0\n",
      "Episode 435: 49.0\n",
      "Episode 436: 72.0\n",
      "Episode 437: 132.0\n",
      "Episode 438: 152.0\n",
      "Episode 439: 200.0\n",
      "Episode 440: 179.0\n",
      "Episode 441: 134.0\n",
      "Episode 442: 54.0\n",
      "Episode 443: 94.0\n",
      "Episode 444: 93.0\n",
      "Episode 445: 82.0\n",
      "Episode 446: 63.0\n",
      "Episode 447: 170.0\n",
      "Episode 448: 82.0\n",
      "Episode 449: 63.0\n",
      "Episode 450: 53.0\n",
      "Episode 451: 69.0\n",
      "Episode 452: 61.0\n",
      "Episode 453: 159.0\n",
      "Episode 454: 126.0\n",
      "Episode 455: 170.0\n",
      "Episode 456: 200.0\n",
      "Episode 457: 187.0\n",
      "Episode 458: 152.0\n",
      "Episode 459: 200.0\n",
      "Episode 460: 200.0\n",
      "Episode 461: 200.0\n",
      "Episode 462: 200.0\n",
      "Episode 463: 200.0\n",
      "Episode 464: 200.0\n",
      "Episode 465: 193.0\n",
      "Episode 466: 140.0\n",
      "Episode 467: 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 468: 200.0\n",
      "Episode 469: 200.0\n",
      "Episode 470: 200.0\n",
      "Episode 471: 200.0\n",
      "Episode 472: 200.0\n",
      "Episode 473: 200.0\n",
      "Episode 474: 200.0\n",
      "Episode 475: 200.0\n",
      "Episode 476: 200.0\n",
      "Episode 477: 200.0\n",
      "Episode 478: 200.0\n",
      "Episode 479: 200.0\n",
      "Episode 480: 200.0\n",
      "Episode 481: 123.0\n",
      "Episode 482: 200.0\n",
      "Episode 483: 193.0\n",
      "Episode 484: 193.0\n",
      "Episode 485: 101.0\n",
      "Episode 486: 144.0\n",
      "Episode 487: 197.0\n",
      "Episode 488: 200.0\n",
      "Episode 489: 159.0\n",
      "Episode 490: 171.0\n",
      "Episode 491: 200.0\n",
      "Episode 492: 125.0\n",
      "Episode 493: 32.0\n",
      "Episode 494: 10.0\n",
      "Episode 495: 10.0\n",
      "Episode 496: 10.0\n",
      "Episode 497: 10.0\n",
      "Episode 498: 8.0\n",
      "Episode 499: 8.0\n",
      "Episode 500: 9.0\n",
      "Episode 501: 9.0\n",
      "Episode 502: 9.0\n",
      "Episode 503: 10.0\n",
      "Episode 504: 10.0\n",
      "Episode 505: 9.0\n",
      "Episode 506: 10.0\n",
      "Episode 507: 10.0\n",
      "Episode 508: 9.0\n",
      "Episode 509: 11.0\n",
      "Episode 510: 9.0\n",
      "Episode 511: 9.0\n",
      "Episode 512: 10.0\n",
      "Episode 513: 9.0\n",
      "Episode 514: 10.0\n",
      "Episode 515: 9.0\n",
      "Episode 516: 10.0\n",
      "Episode 517: 10.0\n",
      "Episode 518: 8.0\n",
      "Episode 519: 10.0\n",
      "Episode 520: 10.0\n",
      "Episode 521: 8.0\n",
      "Episode 522: 9.0\n",
      "Episode 523: 9.0\n",
      "Episode 524: 10.0\n",
      "Episode 525: 9.0\n",
      "Episode 526: 11.0\n",
      "Episode 527: 9.0\n",
      "Episode 528: 10.0\n",
      "Episode 529: 10.0\n",
      "Episode 530: 10.0\n",
      "Episode 531: 9.0\n",
      "Episode 532: 10.0\n",
      "Episode 533: 10.0\n",
      "Episode 534: 10.0\n",
      "Episode 535: 9.0\n",
      "Episode 536: 10.0\n",
      "Episode 537: 10.0\n",
      "Episode 538: 8.0\n",
      "Episode 539: 11.0\n",
      "Episode 540: 9.0\n",
      "Episode 541: 9.0\n",
      "Episode 542: 8.0\n",
      "Episode 543: 10.0\n",
      "Episode 544: 10.0\n",
      "Episode 545: 9.0\n",
      "Episode 546: 8.0\n",
      "Episode 547: 10.0\n",
      "Episode 548: 10.0\n",
      "Episode 549: 9.0\n",
      "Episode 550: 10.0\n",
      "Episode 551: 9.0\n",
      "Episode 552: 8.0\n",
      "Episode 553: 9.0\n",
      "Episode 554: 9.0\n",
      "Episode 555: 9.0\n",
      "Episode 556: 10.0\n",
      "Episode 557: 10.0\n",
      "Episode 558: 10.0\n",
      "Episode 559: 10.0\n",
      "Episode 560: 9.0\n",
      "Episode 561: 9.0\n",
      "Episode 562: 9.0\n",
      "Episode 563: 8.0\n",
      "Episode 564: 8.0\n",
      "Episode 565: 8.0\n",
      "Episode 566: 10.0\n",
      "Episode 567: 10.0\n",
      "Episode 568: 9.0\n",
      "Episode 569: 8.0\n",
      "Episode 570: 9.0\n",
      "Episode 571: 10.0\n",
      "Episode 572: 10.0\n",
      "Episode 573: 9.0\n",
      "Episode 574: 9.0\n",
      "Episode 575: 8.0\n",
      "Episode 576: 8.0\n",
      "Episode 577: 9.0\n",
      "Episode 578: 8.0\n",
      "Episode 579: 9.0\n",
      "Episode 580: 8.0\n",
      "Episode 581: 10.0\n",
      "Episode 582: 8.0\n",
      "Episode 583: 10.0\n",
      "Episode 584: 9.0\n",
      "Episode 585: 10.0\n",
      "Episode 586: 8.0\n",
      "Episode 587: 10.0\n",
      "Episode 588: 10.0\n",
      "Episode 589: 10.0\n",
      "Episode 590: 10.0\n",
      "Episode 591: 9.0\n",
      "Episode 592: 8.0\n",
      "Episode 593: 9.0\n",
      "Episode 594: 10.0\n",
      "Episode 595: 8.0\n",
      "Episode 596: 11.0\n",
      "Episode 597: 9.0\n",
      "Episode 598: 10.0\n",
      "Episode 599: 10.0\n",
      "Episode 600: 9.0\n"
     ]
    }
   ],
   "source": [
    "seed = 2000\n",
    "# CREATING THE ENVIRONMENT\n",
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)\n",
    "env.seed(seed)     # reproducible, general Policy gradient has high variance\n",
    "\n",
    "# INITIALIZING THE Q-PARAMETERS\n",
    "hidden_size = 32\n",
    "max_episodes = 300*2  # Set total number of episodes to train agent on.\n",
    "\n",
    "# train\n",
    "agent = DQNAgent(\n",
    "    env, \n",
    "#     memory_size, \n",
    "#     batch_size, \n",
    "#     epsilon_decay,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.random.set_seed(2000)\n",
    "    # 2.5 TRAINING LOOP\n",
    "    #List to contain all the rewards of all the episodes given to the agent\n",
    "    scores = []\n",
    "    \n",
    "    # EACH EPISODE    \n",
    "    for episode in range(max_episodes):\n",
    "        ## Reset environment and get first new observation\n",
    "        state = agent.env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False  # has the enviroment finished?\n",
    "        \n",
    "            \n",
    "        # EACH TIME STEP    \n",
    "        while not done:\n",
    "        # for step in range(max_steps):  # step index, maximum step is 200\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # TAKING ACTION\n",
    "            next_state, reward, done, _ = agent.env.step(action)\n",
    "            aloss, closs = agent.train_step(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Our new state is state\n",
    "            state = next_state\n",
    "            \n",
    "            episode_reward += reward\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                scores.append(episode_reward)\n",
    "                print(\"Episode \" + str(episode+1) + \": \" + str(episode_reward))\n",
    "                \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a65318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
